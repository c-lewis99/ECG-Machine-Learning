{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout,MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # # More imports\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D, ZeroPadding1D, LSTM, Bidirectional\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cassianlewis/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed Y data\n",
    "Y = pd.read_csv('Datasets/Ydata_all.csv')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200118, 2400)\n",
      "(200118, 200, 12)\n"
     ]
    }
   ],
   "source": [
    "# Loading the X data\n",
    "X=np.loadtxt('Datasets/Xdata_all.csv')\n",
    "print(X.shape)\n",
    "X=X.reshape(len(Y), 200, 12)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting SCP codes from keys to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SR', 'AFIB', 'STACH', 'SARRH', 'SBRAD', 'PACE', 'SVARR', 'BIGU',\n",
       "       'AFLT', 'SVTAC', 'PSVT', 'TRIGU'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the classifications of arrythmias\n",
    "# You may need to create a folder called Data and place all the data within it\n",
    "# Alternatively, change the path\n",
    "path = 'Data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'\n",
    "\n",
    "classifications=pd.read_csv(path+'scp_statements.csv', index_col=0)\n",
    "diagnostics=classifications[classifications.diagnostic==1]\n",
    "forms=classifications[classifications.form==1]\n",
    "rhythms=classifications[classifications.rhythm==1]\n",
    "rhythms.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random plotting function\n",
    "random.seed(1)\n",
    "def plot(X, Y, i, rand, info): \n",
    "    if rand==True:\n",
    "        int=random.randrange(len(X))\n",
    "    else:\n",
    "        int=i\n",
    "    if info==True:\n",
    "        print(Y.iloc[int])\n",
    "    print(int)\n",
    "    plt.plot(X[int,:300, 11])\n",
    "    plt.xlabel('samples')\n",
    "    plt.ylabel('mV/Lead 1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(X, Y, 2, rand=True, info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the superclasses from the scp statements and apply to the database\n",
    "def aggregate_supclass_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in diagnostics.index:\n",
    "#             tmp.append(diagnostics.loc[key].diagnostic_class)\n",
    "#             Only take probabilities of 100%!\n",
    "            if y_dic.get(key)==100:\n",
    "                tmp.append(diagnostics.loc[key].diagnostic_class)\n",
    "    return list(set(tmp))\n",
    "    \n",
    "# Apply diagnostic superclass\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_supclass_diagnostic)\n",
    "Y['diagnostic_superclass_len'] = Y['diagnostic_superclass'].apply(len)\n",
    "# Y_reduced=Y[Y.diagnostic_superclass_len>0]\n",
    "# X_reduced=X[np.where(Y.diagnostic_superclass_len>0)]            \n",
    "# # multi=Y_reduced.loc[Y.diagnostic_superclass_len>0, 'diagnostic_superclass']\n",
    "# multi=Y_reduced['diagnostic_superclass']\n",
    "\n",
    "# # Hash the following line for multiclass ECGs\n",
    "# # This line reduces for example, [NORM, STTC] to [NORM] by taking the first element \n",
    "# Y_reduced['diagnostic_superclass'] = [x[0] for x in multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200118\n"
     ]
    }
   ],
   "source": [
    "print(len(Y))\n",
    "# print(len(Y_reduced))\n",
    "# print(len(Y_reduced[Y_reduced.diagnostic_subclass_len==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the subclasses from the scp statements and apply to the database\n",
    "def aggregate_subclass_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in diagnostics.index:\n",
    "            if y_dic.get(key)==100:\n",
    "                tmp.append(diagnostics.loc[key].diagnostic_subclass)\n",
    "    ret = list(set(tmp))\n",
    "    ret = ['sub_'+r for r in ret] # to distinguish between subclass and superclass columns\n",
    "    return ret\n",
    "\n",
    "# Apply diagnostic subclass\n",
    "Y['diagnostic_subclass'] = Y.scp_codes.apply(aggregate_subclass_diagnostic)\n",
    "Y['diagnostic_subclass_len'] = Y['diagnostic_subclass'].apply(len)\n",
    "\n",
    "# Y_reduced=Y_reduced[Y_reduced.diagnostic_subclass_len>0]\n",
    "# X_reduced=X_reduced[np.where(Y_reduced.diagnostic_subclass_len>0)]  \n",
    "\n",
    "# # multi=Y_reduced.loc[Y_reduced.diagnostic_subclass_len==1, 'diagnostic_subclass']\n",
    "# multi=Y_reduced['diagnostic_subclass']\n",
    "\n",
    "# print(len(multi), len(Y_reduced))\n",
    "\n",
    "\n",
    "# # Hash the following line for multiclass ECGs\n",
    "# Y_reduced['diagnostic_subclass'] = [x[0] for x in multi]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rhythms_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in rhythms.index:\n",
    "                tmp.append(key)\n",
    "    ret = list(set(tmp))\n",
    "    return tmp\n",
    "\n",
    "# Apply rhythms\n",
    "Y['rhythms'] = Y.scp_codes.apply(aggregate_rhythms_diagnostic)\n",
    "Y['rhythms_len'] = Y['rhythms'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take data where Super/subclasses are labelled\n",
    "Y_labelled=Y[Y.diagnostic_superclass_len>0]\n",
    "X_labelled=X[np.where(Y.diagnostic_superclass_len>0)] \n",
    "\n",
    "Y_labelled=Y_labelled[Y_labelled.diagnostic_subclass_len>0]\n",
    "X_labelled=X_labelled[np.where(Y_labelled.diagnostic_subclass_len>0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take data validated by humans (we dont trust robots!)\n",
    "Y_labelled=Y_labelled[Y_labelled.validated_by_human==True]\n",
    "X_labelled=X_labelled[np.where(Y_labelled.validated_by_human==True)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98092\n"
     ]
    }
   ],
   "source": [
    "# Filter data for which only 1 subclass is present for each ECG\n",
    "Y_single_class=Y_labelled[Y_labelled.diagnostic_superclass_len==1]\n",
    "X_single_class=X_labelled[np.where(Y_labelled.diagnostic_superclass_len==1)]\n",
    "print(len(Y_single_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92396\n"
     ]
    }
   ],
   "source": [
    "Y_single_class=Y_single_class[Y_single_class.diagnostic_subclass_len==1]\n",
    "X_single_class=X_single_class[np.where(Y_single_class.diagnostic_subclass_len==1)]\n",
    "print(len(Y_single_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_superclass = pd.Series(np.hstack(Y_single_class['diagnostic_superclass'].values))\n",
    "# all_subclass = pd.Series(np.hstack(Y_single_class['diagnostic_subclass'].values))\n",
    "# print(len(all_subclass))\n",
    "\n",
    "# superclass_cols = all_superclass.unique()\n",
    "# subclass_cols = all_subclass.unique()\n",
    "# update_cols = np.concatenate([superclass_cols, subclass_cols]) # add meta data columns\n",
    "# meta_cols = ['age', 'sex', 'height', 'weight', 'nurse', 'site', 'device',] # could add more columns as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_superclass = pd.Series(np.concatenate(Y_labelled['diagnostic_superclass'].values))\n",
    "all_subclass = pd.Series(np.concatenate(Y_labelled['diagnostic_subclass'].values))\n",
    "all_rhythms = pd.Series(np.concatenate(Y_labelled['rhythms'].values))\n",
    "superclass_cols = all_superclass.unique()\n",
    "subclass_cols = all_subclass.unique()\n",
    "rhythms_cols=all_rhythms.unique()\n",
    "update_cols = np.concatenate([superclass_cols, subclass_cols, rhythms_cols]) # add meta data columns\n",
    "meta_cols = ['age', 'sex', 'height', 'weight', 'nurse', 'site', 'device',] # could add more columns as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassUpdate():\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def __call__(self, row):\n",
    "        for sc in row['diagnostic_superclass']:\n",
    "            row[sc] = 1\n",
    "        for sc in row['diagnostic_subclass']:\n",
    "            row[sc] = 1\n",
    "        for sc in row['rhythms']:\n",
    "            row[sc] = 1\n",
    "            \n",
    "        return row\n",
    "\n",
    "def get_data_by_folds(folds, x, y, update_cols, feature_cols):\n",
    "    assert len(folds)  > 0, '# of provided folds should longer than 1'\n",
    "    #print(y.strat_fold)\n",
    "    filt = np.isin(y.strat_fold.values, folds)\n",
    "    x_selected = x[filt]\n",
    "    y_selected = y[filt]\n",
    "    \n",
    "    for sc in update_cols:\n",
    "        y_selected[sc] = 0\n",
    "        \n",
    "    cls_updt = ClassUpdate(update_cols)\n",
    "    \n",
    "    y_selected = y_selected.apply(cls_updt, axis=1)\n",
    "    \n",
    "    return x_selected, y_selected[list(feature_cols)+list(update_cols)+['strat_fold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, y_all = get_data_by_folds(np.arange(1, 11), X_labelled, Y_labelled, update_cols, meta_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_superclass = pd.Series(np.concatenate(Y_labelled['diagnostic_superclass'].values))\n",
    "# all_subclass = pd.Series(np.concatenate(Y_labelled['diagnostic_subclass'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_superclass=pd.get_dummies(all_superclass)\n",
    "# ohe_subclass=pd.get_dummies(all_subclass)\n",
    "# print(len(ohe_superclass))\n",
    "# # Y_single_class=Y_single_class.join(ohe_superclass)\n",
    "# # Y_single_class=Y_single_class.join(ohe_subclass)\n",
    "# # # Y_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "y_s=y_all.reset_index(drop=True)\n",
    "y_shuffle=y_s.sample(frac=1)\n",
    "x_shuffle=x_all[y_shuffle.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "def norm(x):\n",
    "    return savgol_filter((x-min(x))/(max(x)-min(x)), 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_shuffle)):\n",
    "    for j in range(12):\n",
    "        x=x_shuffle[i,:,j]\n",
    "        x_shuffle[i,:,j]=norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_diagnostics = x_all[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4000_random_not_CRBBB=x_shuffle[:4000,:,:].reshape(4000,2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_all_diagnostics.csv', X_all_diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shuffle.to_csv('Y_all_diagnostics.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shuffle[['NORM', 'CD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_CRBBB=y_shuffle[y_shuffle.sub_CRBBB==1]\n",
    "X_CRBBB=x_shuffle[np.where(y_shuffle.sub_CRBBB==1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_CRBBB_not=y_shuffle[y_shuffle.sub_CRBBB==0][:10000]\n",
    "X_CRBBB_not=x_shuffle[np.where(y_shuffle.sub_CRBBB==0)][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CRBBB_not.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_CRBBB_not.csv', X_CRBBB_not.reshape(10000,2400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CRBBB_reshaped=X_CRBBB.reshape(X_CRBBB.shape[0], 2400)\n",
    "np.savetxt('X_CRBBB.csv', X_CRBBB_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    r=np.random.randint(200)\n",
    "    plt.plot(X_norm[r,:,11])\n",
    "    plt.plot(X_CRBBB[r,:,11])\n",
    "    plt.grid()\n",
    "    plt.xlabel('samples')\n",
    "    plt.ylabel('mV/Lead 1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(X_CRBBB[i,:,11])\n",
    "    plt.xlabel('samples')\n",
    "    plt.ylabel('mV/Lead 1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(X_CRBBB, Y_CRBBB, 1, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_NORM         51425\n",
      "sub_STTC         14793\n",
      "sub_LAFB/LPFB    13011\n",
      "sub_AMI          12697\n",
      "sub_IRBBB         7905\n",
      "sub_IMI           7530\n",
      "sub_ISC_          7222\n",
      "sub_LVH           6623\n",
      "sub_IVCD          5743\n",
      "sub__AVB          5635\n",
      "sub_ISCA          5000\n",
      "sub_NST_          4764\n",
      "sub_CRBBB         3635\n",
      "sub_CLBBB         2992\n",
      "sub_LAO/LAE       2102\n",
      "sub_ISCI          1630\n",
      "sub_RAO/RAE        871\n",
      "sub_ILBBB          597\n",
      "sub_LMI            407\n",
      "sub_RVH            249\n",
      "sub_WPW            200\n",
      "sub_SEHYP          151\n",
      "sub_PMI             52\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# How much data is there for each class?\n",
    "super_nums=y_all[superclass_cols].sum(axis=0)\n",
    "# print(super_nums)\n",
    "\n",
    "sub_nums=y_all[subclass_cols].sum(axis=0)\n",
    "print(sub_nums.sort_values(ascending=False))\n",
    "\n",
    "\n",
    "\n",
    "rhythms_no=y_all[rhythms_cols].sum(axis=0)\n",
    "# print(rhythms_no).sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are now ready to split the data into a format suitable for CNN's: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter(X, Y, class_list, N):\n",
    "    \n",
    "    Y_class=Y[class_list]\n",
    "    y_cat= Y_class[(Y_class[class_list[0]] == 1)]\n",
    "    x_cat= X[np.where((Y_class[class_list[0]] == 1))]\n",
    "    random_index=random.sample(range(0, len(y_cat)), N)\n",
    "    \n",
    "    x_cat=x_cat[random_index]\n",
    "    y_cat=y_cat.iloc[random_index]\n",
    "    \n",
    "    for i in range(1, len(class_list)):\n",
    "        \n",
    "        y_temp= Y_class[(Y_class[class_list[i]] == 1)]\n",
    "        x_temp= X[np.where((Y_class[class_list[i]] == 1))]\n",
    "        \n",
    "        n_positives=len(y_temp)\n",
    "        n_instances=y_cat[(y_cat[class_list[i]] == 1)].sum(axis=0)\n",
    "#         print(class_list[i], n_instances[i])\n",
    "        nn=int(N-n_instances[i])\n",
    "        if nn<0 or N>n_positives:\n",
    "            nn=n_positives\n",
    "        print(nn)\n",
    "        \n",
    "        random_index=random.sample(range(0, len(y_temp)), nn)\n",
    "        x_temp=x_temp[random_index]\n",
    "        y_temp=y_temp.iloc[random_index]          \n",
    "                  \n",
    "        y_cat=pd.concat((y_cat, y_temp), axis=0)\n",
    "        x_cat=np.concatenate((x_cat, x_temp), axis=0)\n",
    "\n",
    "    y_cat=y_cat[class_list]\n",
    "    \n",
    "    \n",
    "    x_unique=np.unique(x_cat, axis=0)\n",
    "    index=np.unique(x_cat, axis=0, return_index=True)[1]\n",
    "    y_unique=y_cat.iloc[index]\n",
    "#     print(y_cat.shape, y_unique.shape)\n",
    "    print(y_cat.sum())\n",
    "    print(y_unique.sum())\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_cat, y_cat, test_size=0.2, shuffle=True)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter1(X, Y, class_list, N):\n",
    "    \n",
    "    Y_class=Y[class_list]\n",
    "    y_cat= Y_class[(Y_class[class_list[0]] == 1)]\n",
    "    x_cat= X[np.where((Y_class[class_list[0]] == 1))]\n",
    "    if len(y_cat)<N:\n",
    "        n=len(y_cat)\n",
    "    else:\n",
    "        n=N\n",
    "    random_index=random.sample(range(0, len(y_cat)), n)\n",
    "    \n",
    "    x_cat=x_cat[random_index]\n",
    "    y_cat=y_cat.iloc[random_index]\n",
    "    \n",
    "    for i in range(1, len(class_list)):\n",
    "        \n",
    "        y_temp= Y_class[(Y_class[class_list[i]] == 1)]\n",
    "        x_temp= X[np.where((Y_class[class_list[i]] == 1))]\n",
    "        \n",
    "        n_positives=len(y_temp)\n",
    "        n_instances=y_cat[(y_cat[class_list[i]] == 1)].sum(axis=0)\n",
    "#         print(class_list[i], n_instances[i])\n",
    "        nn=int(N-n_instances[i])\n",
    "        if nn<0 or N>n_positives:\n",
    "            nn=n_positives\n",
    "        print(nn)\n",
    "        \n",
    "        random_index=random.sample(range(0, len(y_temp)), nn)\n",
    "        x_temp=x_temp[random_index]\n",
    "        y_temp=y_temp.iloc[random_index]          \n",
    "                  \n",
    "        y_cat=pd.concat((y_cat, y_temp), axis=0)\n",
    "        x_cat=np.concatenate((x_cat, x_temp), axis=0)\n",
    "\n",
    "    y_cat=y_cat[class_list]\n",
    "    \n",
    "    \n",
    "    x_unique=np.unique(x_cat, axis=0)\n",
    "    index=np.unique(x_cat, axis=0, return_index=True)[1]\n",
    "    y_unique=y_cat.iloc[index]\n",
    "#     print(y_cat.shape, y_unique.shape)\n",
    "    print(y_cat.sum())\n",
    "    print(y_unique.sum())\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_unique, y_unique, test_size=0.2, shuffle=True)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "sub_NORM    1000\n",
      "sub_AMI     1000\n",
      "dtype: int64\n",
      "sub_NORM    1000\n",
      "sub_AMI     1000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes=['sub_NORM', 'sub_LAFB/LPFB', 'sub_AMI', 'sub_STTC', 'sub_LVH', 'sub_IVCD', 'sub_IRBBB', 'sub__AVB', 'sub_ISCA', 'sub_IMI', 'sub_ISC_', 'sub_NST_']\n",
    "classes1=['sub_LMI', 'sub_RAO/RAE', 'sub_ILBBB', 'sub_LAO/LAE', 'sub_WPW', 'sub_ISCI', 'sub__AVB', 'sub_ISC_', 'sub_IVCD', 'sub_ISCA','sub_CRBBB', 'sub_NST_', 'sub_CLBBB', 'sub_LVH', 'sub_IRBBB', 'sub_LAFB/LPFB', 'sub_IMI', 'sub_AMI', 'sub_STTC', 'sub_NORM'] \n",
    "classes2=['NORM','HYP','MI', 'CD', 'STTC']\n",
    "classes3=['sub_NORM', 'sub_AMI']\n",
    "X_train, X_test, Y_train, Y_test=data_splitter1(x_shuffle, y_shuffle, classes3, 1000)\n",
    "# test=data_splitter_3(x_all, y_all,'sub_NORM', 'sub_AMI','sub_IRBBB',1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871\n",
      "597\n",
      "2102\n",
      "200\n",
      "1630\n",
      "5635\n",
      "7222\n",
      "5743\n",
      "5000\n",
      "3635\n",
      "4764\n",
      "2992\n",
      "6623\n",
      "7905\n",
      "2699\n",
      "7530\n",
      "12697\n",
      "6022\n",
      "9093\n",
      "sub_LMI           1004\n",
      "sub_RAO/RAE       1488\n",
      "sub_ILBBB         1456\n",
      "sub_LAO/LAE       5256\n",
      "sub_WPW            233\n",
      "sub_ISCI          2880\n",
      "sub__AVB         11884\n",
      "sub_ISC_         16207\n",
      "sub_IVCD         10926\n",
      "sub_ISCA          8580\n",
      "sub_CRBBB         5641\n",
      "sub_NST_          5527\n",
      "sub_CLBBB         4055\n",
      "sub_LVH          15436\n",
      "sub_IRBBB        10811\n",
      "sub_LAFB/LPFB    14271\n",
      "sub_IMI          14810\n",
      "sub_AMI          24634\n",
      "sub_STTC         10002\n",
      "sub_NORM         10000\n",
      "dtype: int64\n",
      "sub_LMI            407\n",
      "sub_RAO/RAE        871\n",
      "sub_ILBBB          597\n",
      "sub_LAO/LAE       2102\n",
      "sub_WPW            200\n",
      "sub_ISCI          1630\n",
      "sub__AVB          5635\n",
      "sub_ISC_          7213\n",
      "sub_IVCD          5736\n",
      "sub_ISCA          4999\n",
      "sub_CRBBB         3635\n",
      "sub_NST_          4751\n",
      "sub_CLBBB         2988\n",
      "sub_LVH           6619\n",
      "sub_IRBBB         7904\n",
      "sub_LAFB/LPFB     8952\n",
      "sub_IMI           7524\n",
      "sub_AMI          12697\n",
      "sub_STTC          7986\n",
      "sub_NORM          9843\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_20class, Y_20class =data_splitter1(x_shuffle, y_shuffle, classes1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_20class.csv', X_20class.reshape(X_20class.shape[0], 2400))\n",
    "Y_20class.to_csv('Y_20class.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5114.45\n"
     ]
    }
   ],
   "source": [
    "x=Y_20class.sum().sum()/20\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_train.csv', X_train.reshape(X_train.shape[0], 2400))\n",
    "np.savetxt('X_test.csv', X_test.reshape(X_test.shape[0], 2400))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.to_csv('Y_train.csv', encoding='utf-8', index=False)\n",
    "Y_test.to_csv('Y_test.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     plot(X_train, Y_train, 5, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51425, 200, 12) (51425, 48)\n"
     ]
    }
   ],
   "source": [
    "# Y_norm=y_shuffle[y_shuffle.sub_NORM==1]\n",
    "# X_norm=x_shuffle[np.where(y_shuffle.sub_NORM==1)]\n",
    "\n",
    "# Y_not_norm=y_shuffle[y_shuffle.sub_NORM==0]\n",
    "# X_not_norm=x_shuffle[np.where(y_shuffle.sub_NORM==0)]\n",
    "\n",
    "# print(Y_norm.shape,Y_not_norm.shape)\n",
    "\n",
    "norm_ohe=pd.get_dummies(y_shuffle.NORM)\n",
    "norm_ohe.columns=['Not_norm', 'Norm']\n",
    "X_train_b, X_test_b, Y_train_b, Y_test_b = train_test_split(x_shuffle, norm_ohe, test_size=0.2, random_state=69)\n",
    "\n",
    "Y_norm=y_shuffle[y_shuffle.NORM==1]\n",
    "X_norm=x_shuffle[np.where(y_shuffle.NORM==1)]\n",
    "\n",
    "Y_not_norm=y_shuffle[y_shuffle.NORM==0]\n",
    "X_not_norm=x_shuffle[np.where(y_shuffle.NORM==0)]\n",
    "print(X_norm.shape,Y_norm.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_norm=y_shuffle[y_shuffle.NORM==1]\n",
    "X_norm=x_shuffle[np.where(y_shuffle.NORM==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_data_12=X_norm[:4000,:,:]\n",
    "X_arr_reshaped = X_norm_data_12.reshape(X_norm_data_12.shape[0], -1)\n",
    "np.savetxt('X_norm_12lead.csv', X_arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_not_norm=y_shuffle[y_shuffle.NORM==0]\n",
    "X_not_norm=x_shuffle[np.where(y_shuffle.NORM==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ohe=pd.get_dummies(y_shuffle.NORM)\n",
    "norm_ohe.columns=['Not_norm', 'Norm']\n",
    "y_shuffle=y_shuffle.join(norm_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_n = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_ohe.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can a CNN determine gender from ECG?\n",
    "sex_ohe=pd.get_dummies(y_shuffle['sex'], columns=['M', 'F'])\n",
    "X_train_sex, X_test_sex, Y_train_sex, Y_test_sex=train_test_split(x_shuffle, sex_ohe, test_size=0.2, random_state=3)\n",
    "\n",
    "print(sex_ohe.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_norm.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages=np.array(Y_norm.age)\n",
    "# print(ages)\n",
    "Y_ages = ages[~np.isnan(ages)]\n",
    "X_ages = X_norm[np.where(ages[~np.isnan(ages)])]\n",
    "# print(ages.shape, Y_ages.shape, Y_ages.shape)\n",
    "for i in range(len(Y_ages)):\n",
    "    if Y_ages[i]<40:\n",
    "        Y_ages[i]=0\n",
    "    elif 40<=Y_ages[i]<50:\n",
    "        Y_ages[i]=1\n",
    "    elif 50<=Y_ages[i]<60:\n",
    "        Y_ages[i]=2\n",
    "    elif 60<=Y_ages[i]<70:\n",
    "        Y_ages[i]=3\n",
    "    elif Y_ages[i]>=70:\n",
    "        Y_ages[i]=4\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ohe=pd.get_dummies(Y_ages)\n",
    "print(age_ohe.sum(axis=0))\n",
    "X_train_age, X_test_age, Y_train_age, Y_test_age=train_test_split(X_ages, age_ohe, test_size=0.2, random_state=58)\n",
    "print(X_train_age.shape, Y_train_age.shape)\n",
    "\n",
    "# X_t, Y_t=X_ages[1000:2000], age_ohe[1000:2000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets test some 1D CNN's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN example (basic)\n",
    "model_basic = Sequential()\n",
    "model_basic.add(Convolution1D(100, 5, activation='relu', input_shape=(200,12)))\n",
    "model_basic.add(Convolution1D(100, 10, activation='relu'))\n",
    "model_basic.add(MaxPooling1D(3))\n",
    "model_basic.add(Convolution1D(100, 10, activation='relu'))\n",
    "model_basic.add(Convolution1D(160, 10, activation='relu'))\n",
    "model_basic.add(GlobalAveragePooling1D())\n",
    "model_basic.add(Dropout(0.2))\n",
    "# model_basic.add(Flatten())\n",
    "model_basic.add(Dense(100, activation='relu'))\n",
    "model_basic.add(Dense(5, activation='sigmoid'))\n",
    "# print(model.summary())\n",
    "model_basic.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "    name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    "    name=\"AUC\",\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=True,\n",
    "    label_weights=None,\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_basic=model_basic.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN example for single class samples\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional block 1\n",
    "model.add(Convolution1D(32, 3, activation='relu', input_shape=(200,12)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Convolution1D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Convolutional block 2\n",
    "model.add(Convolution1D(64, 5, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Convolution1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "# Convolutional block 3\n",
    "model.add(Convolution1D(128, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "# model.add(Convolution1D(128, 7, activation='relu'))\n",
    "# model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "    name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    "    name=\"AUC\",\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=True,\n",
    "    label_weights=None,\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN example for multiclass samples\n",
    "# model_1 = Sequential()\n",
    "\n",
    "# # Convolutional block 1\n",
    "# model_1.add(Convolution1D(32, 3, activation='relu', input_shape=(200,12)))\n",
    "# model_1.add(tf.keras.layers.BatchNormalization())\n",
    "# model_1.add(MaxPooling1D(2))\n",
    "# model_1.add(Convolution1D(32, 3, activation='relu'))\n",
    "# model_1.add(MaxPooling1D(2))\n",
    "# model_1.add(Dropout(0.3))\n",
    "\n",
    "# # Convolutional block 2\n",
    "# model_1.add(Convolution1D(64, 5, activation='relu'))\n",
    "# model_1.add(MaxPooling1D(2))\n",
    "# model_1.add(Convolution1D(64, 5, activation='relu'))\n",
    "# model_1.add(MaxPooling1D(2))\n",
    "\n",
    "# # Convolutional block 3\n",
    "# model_1.add(Convolution1D(128, 7, activation='relu'))\n",
    "# model_1.add(MaxPooling1D(2))\n",
    "# # model.add(Convolution1D(128, 7, activation='relu'))\n",
    "# # model.add(MaxPooling1D(2))\n",
    "\n",
    "# model_1.add(Flatten())\n",
    "# model_1.add(Dropout(0.3))\n",
    "# model_1.add(Dense(64, activation='relu'))\n",
    "# model_1.add(Dropout(0.3))\n",
    "# model_1.add(Dense(20, activation='relu'))\n",
    "# model_1.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "# # print(model_1.summary())\n",
    "# model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_no=50\n",
    "# Model_1=model_1.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does amount of data affect accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyRelu model\n",
    "model_1= Sequential()\n",
    "model_1.add(Convolution1D(32,3, input_shape=(200,12)))\n",
    "model_1.add(LeakyReLU(alpha=0.01))\n",
    "model_1.add(MaxPooling1D(2))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "model_1.add(Convolution1D(64,3))\n",
    "model_1.add(LeakyReLU(alpha=0.01))\n",
    "model_1.add(MaxPooling1D(2))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(256))\n",
    "model_1.add(LeakyReLU(alpha=0.01))\n",
    "model_1.add(Dropout(0.25))\n",
    "model_1.add(Dense(2, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyRelu model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Convolution1D(32,1, input_shape=(200,12)))\n",
    "model_2.add(LeakyReLU(alpha=0.01))\n",
    "model_2.add(MaxPooling1D(2))\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Convolution1D(64,1))\n",
    "model_2.add(LeakyReLU(alpha=0.01))\n",
    "model_2.add(MaxPooling1D(2))\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(256))\n",
    "model_2.add(LeakyReLU(alpha=0.01))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(Dense(2, activation='sigmoid'))\n",
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "    name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    "    name=\"AUC\",\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=True,\n",
    "    label_weights=None,\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4764\n",
      "sub_NORM    5000\n",
      "sub_NST_    4764\n",
      "dtype: int64\n",
      "sub_NORM    5000\n",
      "sub_NST_    4751\n",
      "dtype: int64\n",
      "Epoch 1/50\n",
      "245/245 [==============================] - 8s 23ms/step - loss: 0.6804 - accuracy: 0.5775 - Recall: 0.5670 - Precision: 0.5791 - AUC: 0.5973 - val_loss: 0.6771 - val_accuracy: 0.5794 - val_Recall: 0.5673 - val_Precision: 0.5813 - val_AUC: 0.6088\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.6689 - accuracy: 0.5960 - Recall: 0.5936 - Precision: 0.5964 - AUC: 0.6256 - val_loss: 0.6692 - val_accuracy: 0.5911 - val_Recall: 0.5986 - val_Precision: 0.5898 - val_AUC: 0.6290\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.6620 - accuracy: 0.6063 - Recall: 0.6123 - Precision: 0.6051 - AUC: 0.6409 - val_loss: 0.6655 - val_accuracy: 0.5881 - val_Recall: 0.5955 - val_Precision: 0.5868 - val_AUC: 0.6378\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.6545 - accuracy: 0.6134 - Recall: 0.6155 - Precision: 0.6130 - AUC: 0.6539 - val_loss: 0.6540 - val_accuracy: 0.6152 - val_Recall: 0.6196 - val_Precision: 0.6142 - val_AUC: 0.6643\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.6448 - accuracy: 0.6250 - Recall: 0.6254 - Precision: 0.6249 - AUC: 0.6703 - val_loss: 0.6513 - val_accuracy: 0.6244 - val_Recall: 0.6129 - val_Precision: 0.6274 - val_AUC: 0.6709\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.6299 - accuracy: 0.6401 - Recall: 0.6429 - Precision: 0.6393 - AUC: 0.6949 - val_loss: 0.6455 - val_accuracy: 0.6231 - val_Recall: 0.6216 - val_Precision: 0.6235 - val_AUC: 0.6735\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.6190 - accuracy: 0.6552 - Recall: 0.6563 - Precision: 0.6548 - AUC: 0.7108 - val_loss: 0.6201 - val_accuracy: 0.6733 - val_Recall: 0.6641 - val_Precision: 0.6766 - val_AUC: 0.7287\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 5s 22ms/step - loss: 0.6023 - accuracy: 0.6731 - Recall: 0.6703 - Precision: 0.6740 - AUC: 0.7345 - val_loss: 0.6093 - val_accuracy: 0.6787 - val_Recall: 0.6687 - val_Precision: 0.6823 - val_AUC: 0.7404\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.5783 - accuracy: 0.6946 - Recall: 0.6944 - Precision: 0.6947 - AUC: 0.7652 - val_loss: 0.5988 - val_accuracy: 0.6797 - val_Recall: 0.6774 - val_Precision: 0.6806 - val_AUC: 0.7432\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.5552 - accuracy: 0.7090 - Recall: 0.7094 - Precision: 0.7088 - AUC: 0.7863 - val_loss: 0.5824 - val_accuracy: 0.6925 - val_Recall: 0.6938 - val_Precision: 0.6920 - val_AUC: 0.7613\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.5368 - accuracy: 0.7278 - Recall: 0.7269 - Precision: 0.7282 - AUC: 0.8047 - val_loss: 0.5985 - val_accuracy: 0.6728 - val_Recall: 0.6703 - val_Precision: 0.6737 - val_AUC: 0.7504\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.5108 - accuracy: 0.7462 - Recall: 0.7452 - Precision: 0.7467 - AUC: 0.8271 - val_loss: 0.5622 - val_accuracy: 0.6943 - val_Recall: 0.6953 - val_Precision: 0.6939 - val_AUC: 0.7839\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.4880 - accuracy: 0.7694 - Recall: 0.7674 - Precision: 0.7704 - AUC: 0.8464 - val_loss: 0.5279 - val_accuracy: 0.7455 - val_Recall: 0.7419 - val_Precision: 0.7473 - val_AUC: 0.8191\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.4615 - accuracy: 0.7830 - Recall: 0.7821 - Precision: 0.7835 - AUC: 0.8646 - val_loss: 0.5336 - val_accuracy: 0.7432 - val_Recall: 0.7404 - val_Precision: 0.7446 - val_AUC: 0.8188\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.4388 - accuracy: 0.7993 - Recall: 0.7987 - Precision: 0.7997 - AUC: 0.8802 - val_loss: 0.5059 - val_accuracy: 0.7652 - val_Recall: 0.7640 - val_Precision: 0.7659 - val_AUC: 0.8361\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.4108 - accuracy: 0.8143 - Recall: 0.8160 - Precision: 0.8132 - AUC: 0.8964 - val_loss: 0.5118 - val_accuracy: 0.7509 - val_Recall: 0.7506 - val_Precision: 0.7510 - val_AUC: 0.8348\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 5s 22ms/step - loss: 0.3869 - accuracy: 0.8296 - Recall: 0.8310 - Precision: 0.8287 - AUC: 0.9099 - val_loss: 0.5062 - val_accuracy: 0.7616 - val_Recall: 0.7568 - val_Precision: 0.7642 - val_AUC: 0.8398\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 0.3697 - accuracy: 0.8405 - Recall: 0.8397 - Precision: 0.8410 - AUC: 0.9175 - val_loss: 0.4995 - val_accuracy: 0.7698 - val_Recall: 0.7660 - val_Precision: 0.7719 - val_AUC: 0.8514\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.3450 - accuracy: 0.8466 - Recall: 0.8466 - Precision: 0.8465 - AUC: 0.9286 - val_loss: 0.4971 - val_accuracy: 0.7714 - val_Recall: 0.7716 - val_Precision: 0.7712 - val_AUC: 0.8565\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 5s 22ms/step - loss: 0.3171 - accuracy: 0.8685 - Recall: 0.8693 - Precision: 0.8678 - AUC: 0.9416 - val_loss: 0.4914 - val_accuracy: 0.7760 - val_Recall: 0.7757 - val_Precision: 0.7761 - val_AUC: 0.8604\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.2994 - accuracy: 0.8729 - Recall: 0.8721 - Precision: 0.8734 - AUC: 0.9471 - val_loss: 0.4638 - val_accuracy: 0.8031 - val_Recall: 0.8018 - val_Precision: 0.8039 - val_AUC: 0.8740\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.2814 - accuracy: 0.8822 - Recall: 0.8821 - Precision: 0.8823 - AUC: 0.9536 - val_loss: 0.4545 - val_accuracy: 0.7988 - val_Recall: 0.7993 - val_Precision: 0.7985 - val_AUC: 0.8821\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.2744 - accuracy: 0.8886 - Recall: 0.8895 - Precision: 0.8878 - AUC: 0.9561 - val_loss: 0.4576 - val_accuracy: 0.7970 - val_Recall: 0.7937 - val_Precision: 0.7990 - val_AUC: 0.8772\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.2450 - accuracy: 0.9035 - Recall: 0.9028 - Precision: 0.9040 - AUC: 0.9657 - val_loss: 0.4349 - val_accuracy: 0.8193 - val_Recall: 0.8208 - val_Precision: 0.8183 - val_AUC: 0.8919\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.2268 - accuracy: 0.9102 - Recall: 0.9097 - Precision: 0.9106 - AUC: 0.9711 - val_loss: 0.4459 - val_accuracy: 0.8149 - val_Recall: 0.8152 - val_Precision: 0.8147 - val_AUC: 0.8888\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.2171 - accuracy: 0.9169 - Recall: 0.9172 - Precision: 0.9167 - AUC: 0.9730 - val_loss: 0.4698 - val_accuracy: 0.8041 - val_Recall: 0.8049 - val_Precision: 0.8037 - val_AUC: 0.8801\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.2010 - accuracy: 0.9226 - Recall: 0.9231 - Precision: 0.9222 - AUC: 0.9772 - val_loss: 0.4456 - val_accuracy: 0.8198 - val_Recall: 0.8203 - val_Precision: 0.8194 - val_AUC: 0.8910\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1948 - accuracy: 0.9236 - Recall: 0.9236 - Precision: 0.9236 - AUC: 0.9782 - val_loss: 0.4299 - val_accuracy: 0.8254 - val_Recall: 0.8264 - val_Precision: 0.8247 - val_AUC: 0.8974\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1781 - accuracy: 0.9334 - Recall: 0.9333 - Precision: 0.9334 - AUC: 0.9817 - val_loss: 0.4406 - val_accuracy: 0.8338 - val_Recall: 0.8341 - val_Precision: 0.8337 - val_AUC: 0.9006\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1737 - accuracy: 0.9325 - Recall: 0.9318 - Precision: 0.9331 - AUC: 0.9829 - val_loss: 0.4421 - val_accuracy: 0.8351 - val_Recall: 0.8351 - val_Precision: 0.8351 - val_AUC: 0.8990\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1592 - accuracy: 0.9430 - Recall: 0.9430 - Precision: 0.9429 - AUC: 0.9858 - val_loss: 0.4509 - val_accuracy: 0.8200 - val_Recall: 0.8208 - val_Precision: 0.8195 - val_AUC: 0.8979\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 5s 19ms/step - loss: 0.1583 - accuracy: 0.9406 - Recall: 0.9412 - Precision: 0.9400 - AUC: 0.9857 - val_loss: 0.4764 - val_accuracy: 0.8292 - val_Recall: 0.8300 - val_Precision: 0.8287 - val_AUC: 0.9043\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 5s 19ms/step - loss: 0.1421 - accuracy: 0.9451 - Recall: 0.9451 - Precision: 0.9452 - AUC: 0.9887 - val_loss: 0.4385 - val_accuracy: 0.8387 - val_Recall: 0.8372 - val_Precision: 0.8398 - val_AUC: 0.9081\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 5s 19ms/step - loss: 0.1278 - accuracy: 0.9539 - Recall: 0.9538 - Precision: 0.9540 - AUC: 0.9908 - val_loss: 0.4544 - val_accuracy: 0.8333 - val_Recall: 0.8336 - val_Precision: 0.8332 - val_AUC: 0.9118\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 6s 23ms/step - loss: 0.1360 - accuracy: 0.9496 - Recall: 0.9501 - Precision: 0.9492 - AUC: 0.9893 - val_loss: 0.4629 - val_accuracy: 0.8321 - val_Recall: 0.8300 - val_Precision: 0.8334 - val_AUC: 0.9067\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1253 - accuracy: 0.9526 - Recall: 0.9528 - Precision: 0.9525 - AUC: 0.9910 - val_loss: 0.4539 - val_accuracy: 0.8415 - val_Recall: 0.8408 - val_Precision: 0.8421 - val_AUC: 0.9074\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1184 - accuracy: 0.9587 - Recall: 0.9585 - Precision: 0.9589 - AUC: 0.9917 - val_loss: 0.4648 - val_accuracy: 0.8408 - val_Recall: 0.8413 - val_Precision: 0.8404 - val_AUC: 0.9097\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1115 - accuracy: 0.9586 - Recall: 0.9584 - Precision: 0.9588 - AUC: 0.9929 - val_loss: 0.4789 - val_accuracy: 0.8387 - val_Recall: 0.8387 - val_Precision: 0.8387 - val_AUC: 0.9107\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1119 - accuracy: 0.9573 - Recall: 0.9571 - Precision: 0.9575 - AUC: 0.9927 - val_loss: 0.4802 - val_accuracy: 0.8451 - val_Recall: 0.8443 - val_Precision: 0.8456 - val_AUC: 0.9145\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1026 - accuracy: 0.9639 - Recall: 0.9639 - Precision: 0.9639 - AUC: 0.9937 - val_loss: 0.4974 - val_accuracy: 0.8433 - val_Recall: 0.8443 - val_Precision: 0.8426 - val_AUC: 0.9099\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.1062 - accuracy: 0.9592 - Recall: 0.9590 - Precision: 0.9593 - AUC: 0.9933 - val_loss: 0.4621 - val_accuracy: 0.8507 - val_Recall: 0.8505 - val_Precision: 0.8509 - val_AUC: 0.9136\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.1068 - accuracy: 0.9611 - Recall: 0.9610 - Precision: 0.9612 - AUC: 0.9933 - val_loss: 0.4702 - val_accuracy: 0.8446 - val_Recall: 0.8438 - val_Precision: 0.8451 - val_AUC: 0.9213\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.0878 - accuracy: 0.9691 - Recall: 0.9691 - Precision: 0.9691 - AUC: 0.9956 - val_loss: 0.4785 - val_accuracy: 0.8454 - val_Recall: 0.8449 - val_Precision: 0.8457 - val_AUC: 0.9173\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.0871 - accuracy: 0.9667 - Recall: 0.9668 - Precision: 0.9666 - AUC: 0.9955 - val_loss: 0.4716 - val_accuracy: 0.8538 - val_Recall: 0.8541 - val_Precision: 0.8536 - val_AUC: 0.9157\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.0758 - accuracy: 0.9731 - Recall: 0.9732 - Precision: 0.9730 - AUC: 0.9968 - val_loss: 0.5120 - val_accuracy: 0.8374 - val_Recall: 0.8367 - val_Precision: 0.8379 - val_AUC: 0.9049\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.0959 - accuracy: 0.9677 - Recall: 0.9679 - Precision: 0.9675 - AUC: 0.9941 - val_loss: 0.4787 - val_accuracy: 0.8361 - val_Recall: 0.8346 - val_Precision: 0.8372 - val_AUC: 0.9147\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.0773 - accuracy: 0.9735 - Recall: 0.9735 - Precision: 0.9735 - AUC: 0.9965 - val_loss: 0.5225 - val_accuracy: 0.8349 - val_Recall: 0.8346 - val_Precision: 0.8350 - val_AUC: 0.9125\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.0682 - accuracy: 0.9774 - Recall: 0.9773 - Precision: 0.9775 - AUC: 0.9974 - val_loss: 0.4912 - val_accuracy: 0.8513 - val_Recall: 0.8515 - val_Precision: 0.8511 - val_AUC: 0.9155\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.0795 - accuracy: 0.9729 - Recall: 0.9731 - Precision: 0.9726 - AUC: 0.9956 - val_loss: 0.4932 - val_accuracy: 0.8466 - val_Recall: 0.8469 - val_Precision: 0.8465 - val_AUC: 0.9209\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 5s 20ms/step - loss: 0.0693 - accuracy: 0.9776 - Recall: 0.9779 - Precision: 0.9774 - AUC: 0.9967 - val_loss: 0.4971 - val_accuracy: 0.8474 - val_Recall: 0.8469 - val_Precision: 0.8478 - val_AUC: 0.9195\n"
     ]
    }
   ],
   "source": [
    "# sub_STTC         14793\n",
    "# sub_NST_          4764\n",
    "# sub_LVH           6623\n",
    "# sub_LAFB/LPFB \n",
    "\n",
    "c=['sub_NORM', 'sub_NST_']\n",
    "X_train, X_test, Y_train, Y_test=data_splitter(x_shuffle, y_shuffle, c, 5000)\n",
    "epoch_no=50\n",
    "Model_2=model_2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyRelu model\n",
    "model_3 = Sequential()\n",
    "model_3.add(Convolution1D(32,1, input_shape=(200,12)))\n",
    "model_3.add(LeakyReLU(alpha=0.01))\n",
    "model_3.add(MaxPooling1D(2))\n",
    "model_3.add(Dropout(0.1))\n",
    "\n",
    "model_3.add(Convolution1D(64,3))\n",
    "model_3.add(LeakyReLU(alpha=0.01))\n",
    "model_3.add(MaxPooling1D(2))\n",
    "model_3.add(Dropout(0.1))\n",
    "\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(55))\n",
    "model_3.add(LeakyReLU(alpha=0.01))\n",
    "model_3.add(Dense(5, activation='sigmoid'))\n",
    "model_2.add(Dropout(0.1))\n",
    "optimizer =tf.keras.optimizers.Adam(clipvalue=0.5)\n",
    "model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_no=50\n",
    "Model_3=model_3.fit(X_train_b, Y_train_b, validation_data=(X_test_b, Y_test_b), epochs=epoch_no, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_no=20\n",
    "# Model_3=model_3.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_1, val_accuracy_1=Model_1.history['accuracy'], Model_1.history['val_accuracy']\n",
    "accuracy_2, val_accuracy_2=Model_2.history['accuracy'], Model_2.history['val_accuracy']\n",
    "accuracy_3, val_accuracy_3=Model_3.history['accuracy'], Model_3.history['val_accuracy']\n",
    "# plt.figure()\n",
    "# plt.plot(range(epoch_no), accuracy_2, label='training 2', color='g')\n",
    "# plt.plot(range(epoch_no), accuracy_3, label='training accuracy', color='b')\n",
    "plt.plot(range(epoch_no), val_accuracy_2, label='dropout=0.10', linestyle='-', color='blue')\n",
    "plt.plot(range(epoch_no), val_accuracy_1, label='dropout=0.25', linestyle='-', color='red')\n",
    "plt.plot(range(epoch_no), val_accuracy_3, label='dropout=0.40', linestyle='-', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Variation of test accuracy by dropout rate in a binary classification task')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning parameters: Optimizer, Learning rate, dropout, LeakyReLu alpha - will only be using 1000 data points to start to reduce time taken to run all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "def create_model(learn_rate=0.01):\n",
    "    # LeakyRelu model\n",
    "    model_3 = Sequential()\n",
    "    model_3.add(Convolution1D(32,3, input_shape=(200,12)))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(0.25))\n",
    "\n",
    "    model_3.add(Convolution1D(64,3))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(0.25))\n",
    "\n",
    "    model_3.add(Flatten())\n",
    "    model_3.add(Dense(256))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    optimizer =tf.keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "    model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=1)\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(learn_rate=learn_rate)\n",
    "grid = GridSearchCV(estimator=model_3, param_grid=param_grid, n_jobs=1, cv=3, verbose=2)\n",
    "grid_result = grid.fit(X_train, Y_train, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"accuracy - %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernal initializer\n",
    "def create_model(init_mode='uniform'):\n",
    "    # LeakyRelu model\n",
    "    model_3 = Sequential()\n",
    "    model_3.add(Convolution1D(32,3, input_shape=(200,12)))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(0.25))\n",
    "\n",
    "    model_3.add(Convolution1D(64,3))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(0.25))\n",
    "\n",
    "    model_3.add(Flatten())\n",
    "    model_3.add(Dense(256, kernel_initializer=init_mode))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(Dense(2, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "    \n",
    "    optimizer =tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = KerasClassifier(build_fn=create_model, epochs=20, batch_size=10, verbose=1)\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model_3, param_grid=param_grid, n_jobs=1, cv=2, verbose=2)\n",
    "grid_result = grid.fit(X_train, Y_train, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"accuracy - %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "def create_model(dropout=0):\n",
    "    # LeakyRelu model\n",
    "    model_3 = Sequential()\n",
    "    model_3.add(Convolution1D(32,3, input_shape=(200,12)))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(dropout))\n",
    "\n",
    "    model_3.add(Convolution1D(64,3))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(dropout))\n",
    "\n",
    "    model_3.add(Flatten())\n",
    "    model_3.add(Dense(256))\n",
    "    model_3.add(LeakyReLU(alpha=0.01))\n",
    "    model_3.add(Dropout(dropout))\n",
    "    model_3.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    optimizer =tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = KerasClassifier(build_fn=create_model, epochs=50, verbose=1)\n",
    "dropout = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "param_grid = dict(dropout=dropout)\n",
    "grid = GridSearchCV(estimator=model_3, param_grid=param_grid, n_jobs=1, cv=4, verbose=2)\n",
    "grid_result = grid.fit(X_train, Y_train, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"accuracy - %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha\n",
    "def create_model(alpha=0):\n",
    "    # LeakyRelu model\n",
    "    model_3 = Sequential()\n",
    "    model_3.add(Convolution1D(32,3, input_shape=(200,12)))\n",
    "    model_3.add(LeakyReLU(alpha=alpha))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(0.1))\n",
    "\n",
    "    model_3.add(Convolution1D(64,3))\n",
    "    model_3.add(LeakyReLU(alpha=alpha))\n",
    "    model_3.add(MaxPooling1D(2))\n",
    "    model_3.add(Dropout(0.1))\n",
    "\n",
    "    model_3.add(Flatten())\n",
    "    model_3.add(Dense(256))\n",
    "    model_3.add(LeakyReLU(alpha=alpha))\n",
    "    model_3.add(Dropout(0.1))\n",
    "    model_3.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = KerasClassifier(build_fn=create_model, epochs=40, verbose=1)\n",
    "alpha = [0.001, 0.003, 0.007, 0.01, 0.03, 0.07, 0.1]\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid = GridSearchCV(estimator=model_3, param_grid=param_grid, n_jobs=1, cv=3, verbose=2)\n",
    "grid_result = grid.fit(X_train, Y_train, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"accuracy - %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, val_accuracy=Model.history['accuracy'], Model.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, val_accuracy=Model_2.history['accuracy'], Model_2.history['val_accuracy']\n",
    "plt.figure()\n",
    "plt.plot(range(epoch_no), accuracy, label='training')\n",
    "plt.plot(range(epoch_no), val_accuracy, label='validation', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss=Model_2.history['loss'], Model_2.history['val_loss']\n",
    "plt.figure()\n",
    "plt.plot(range(epoch_no), loss, label='training')\n",
    "plt.plot(range(epoch_no), val_loss, label='validation', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score= model.evaluate(X_test, Y_test)\n",
    "# print(X_test.shape, Y_test.shape, Y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=model.predict(X_test) \n",
    "Y_pred=np.argmax(predict_x,axis=1)\n",
    "print(Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.array(Y_test)\n",
    "true=np.argmax(t, axis=1)\n",
    "# for i in range(len(t)):\n",
    "#     Y_test['int']=t.iloc[i].argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true, Y_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexNet_model=Sequential()\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=96, kernel_size=11, strides=4, input_shape=(200,12)))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=256, kernel_size=5, padding='same'))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=384, padding='same', kernel_size=3))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "alexNet_model.add(Convolution1D(filters=384, kernel_size=3))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=256, kernel_size=3))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model.add(GlobalAveragePooling1D())\n",
    "alexNet_model.add(Dense(128, activation='relu'))\n",
    "alexNet_model.add(Dropout(0.2))\n",
    "alexNet_model.add(Dense(128, activation='relu'))\n",
    "alexNet_model.add(Dropout(0.2))\n",
    "alexNet_model.add(Dense(5, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " alexNet_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_no=50\n",
    "alex_model=alexNet_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyRelu model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Convolution1D(32,3, input_shape=(200,12)))\n",
    "model_2.add(LeakyReLU(alpha=0.01))\n",
    "model_2.add(MaxPooling1D(2))\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Convolution1D(64,3))\n",
    "model_2.add(LeakyReLU(alpha=0.01))\n",
    "model_2.add(MaxPooling1D(2))\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(256))\n",
    "model_2.add(LeakyReLU(alpha=0.01))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(Dense(4, activation='sigmoid'))\n",
    "# print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_no=50\n",
    "Model_2=model_2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_alex, val_acc_alex=alex_model_1.history['accuracy'], alex_model_1.history['val_accuracy']\n",
    "# accuracy, val_accuracy=Model_2.history['accuracy'], Model_2.history['val_accuracy']\n",
    "plt.figure()\n",
    "plt.plot(range(1, epoch_no+1), val_acc_alex, label='AlexNet')\n",
    "# plt.plot(range(1, epoch_no+1), val_accuracy, label='LeakyRelu')\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('5 Class Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=alexNet_model.predict(X_test) \n",
    "print(predict_x)\n",
    "# predict_x_2=model_2.predict(X_test) \n",
    "# print(predict_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=predict_x\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    for j in range(Y_pred.shape[1]):\n",
    "        if Y_pred[i,j]>0.5:\n",
    "            Y_pred[i,j]=1\n",
    "        else:\n",
    "            Y_pred[i,j]=0\n",
    "            \n",
    "# Y_pred_2=predict_x_2\n",
    "# for i in range(Y_pred_2.shape[0]):\n",
    "#     for j in range(Y_pred_2.shape[1]):\n",
    "#         if Y_pred_2[i,j]>0.5:\n",
    "#             Y_pred_2[i,j]=1\n",
    "#         else:\n",
    "#             Y_pred_2[i,j]=0\n",
    "                \n",
    "print(Y_pred[200:210]) \n",
    "print(Y_test[200:210])\n",
    "# print(Y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "counter=0\n",
    "n_labels=0\n",
    "Y_test_arr=np.array(Y_test)\n",
    "total=Y_pred.shape[0]*Y_pred.shape[1]\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    for j in range(Y_pred.shape[1]):\n",
    "#         if Y_test_arr[i,j]==1:\n",
    "        n_labels+=1\n",
    "        if Y_pred[i,j]==Y_test_arr[i,j]:\n",
    "            counter+=1\n",
    "\n",
    "print(100*counter/n_labels)        \n",
    "print(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "n_labels=0\n",
    "Y_test_arr=np.array(Y_test)\n",
    "total=Y_pred.shape[0]*Y_pred.shape[1]\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    for j in range(Y_pred.shape[1]):\n",
    "        if Y_test_arr[i,j]==0:\n",
    "            n_labels+=1\n",
    "            if Y_pred[i,j]==Y_test_arr[i,j]:\n",
    "                counter+=1\n",
    "            \n",
    "print(100*counter/n_labels)\n",
    "print(n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search for the AlexNet CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def alex_model(dropout_rate=0):\n",
    "    alexNet_model=Sequential()\n",
    "\n",
    "    alexNet_model.add(Convolution1D(filters=96, kernel_size=11, strides=4, input_shape=(200,12)))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "    alexNet_model.add(Convolution1D(filters=256, kernel_size=5, padding='same'))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "    alexNet_model.add(Convolution1D(filters=384, padding='same', kernel_size=3))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(Convolution1D(filters=384, kernel_size=3))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(Convolution1D(filters=256, kernel_size=3))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "    alexNet_model.add(GlobalAveragePooling1D())\n",
    "    alexNet_model.add(Dense(128, activation='relu'))\n",
    "    alexNet_model.add(Dropout(dropout_rate))\n",
    "    alexNet_model.add(Dense(128, activation='relu'))\n",
    "    alexNet_model.add(Dropout(dropout_rate))\n",
    "    alexNet_model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    alexNet_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "    \n",
    "    return alexNet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test=data_splitter(x_shuffle, y_shuffle, ['sub_IRBBB', 'sub_AMI'], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_drop = KerasClassifier(build_fn=alex_model, epochs=40, verbose=1)\n",
    "dropout_rate = [0, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(dropout_rate=dropout_rate)\n",
    "grid = GridSearchCV(estimator=alex_drop, param_grid=param_grid, n_jobs=1, cv=2, verbose=2)\n",
    "grid_result = grid.fit(X_train, Y_train, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"accuracy - %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alex_model_op(optimizer='adam'):\n",
    "    alexNet_model=Sequential()\n",
    "\n",
    "    alexNet_model.add(Convolution1D(filters=96, kernel_size=11, strides=4, input_shape=(200,12)))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "    alexNet_model.add(Convolution1D(filters=256, kernel_size=5, padding='same'))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "    alexNet_model.add(Convolution1D(filters=384, padding='same', kernel_size=3))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(Convolution1D(filters=384, kernel_size=3))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(Convolution1D(filters=256, kernel_size=3))\n",
    "    alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "    alexNet_model.add(Activation('relu'))\n",
    "    alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "    alexNet_model.add(GlobalAveragePooling1D())\n",
    "    alexNet_model.add(Dense(128, activation='relu'))\n",
    "    alexNet_model.add(Dropout(0.2))\n",
    "    alexNet_model.add(Dense(128, activation='relu'))\n",
    "    alexNet_model.add(Dropout(0.2))\n",
    "    alexNet_model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    alexNet_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=optimizer, metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "    \n",
    "    return alexNet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_opt = KerasClassifier(build_fn=alex_model_op, epochs=40, verbose=1)\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=alex_opt, param_grid=param_grid, n_jobs=1, cv=2, verbose=2)\n",
    "grid_result = grid.fit(X_train, Y_train, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"accuracy - %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexNet_model=Sequential()\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=96, kernel_size=11, strides=4, input_shape=(200,12)))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=256, kernel_size=5, padding='same'))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=384, padding='same', kernel_size=3))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=384, kernel_size=3))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "\n",
    "alexNet_model.add(Convolution1D(filters=256, kernel_size=3))\n",
    "alexNet_model.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model.add(Activation('relu'))\n",
    "\n",
    "alexNet_model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model.add(GlobalAveragePooling1D())\n",
    "alexNet_model.add(Dense(128, activation='relu'))\n",
    "alexNet_model.add(Dropout(0.1))\n",
    "alexNet_model.add(Dense(128, activation='relu'))\n",
    "alexNet_model.add(Dropout(0.1))\n",
    "alexNet_model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "alexNet_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "    name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    "    name=\"AUC\",\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=True,\n",
    "    label_weights=None,\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_no=70\n",
    "alex_model_1=alexNet_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexNet_model_2=Sequential()\n",
    "\n",
    "alexNet_model_2.add(Convolution1D(filters=96, kernel_size=11, strides=4, input_shape=(200,12)))\n",
    "alexNet_model_2.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model_2.add(Activation('relu'))\n",
    "alexNet_model_2.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model_2.add(Convolution1D(filters=256, kernel_size=5, padding='same'))\n",
    "alexNet_model_2.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model_2.add(Activation('relu'))\n",
    "alexNet_model_2.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model_2.add(Convolution1D(filters=384, padding='same', kernel_size=3))\n",
    "alexNet_model_2.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model_2.add(Activation('relu'))\n",
    "alexNet_model_2.add(Convolution1D(filters=384, kernel_size=3))\n",
    "alexNet_model_2.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model_2.add(Activation('relu'))\n",
    "alexNet_model_2.add(Convolution1D(filters=256, kernel_size=3))\n",
    "alexNet_model_2.add(tf.keras.layers.BatchNormalization())\n",
    "alexNet_model_2.add(Activation('relu'))\n",
    "alexNet_model_2.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "alexNet_model_2.add(GlobalAveragePooling1D())\n",
    "alexNet_model_2.add(Dense(128, activation='relu'))\n",
    "alexNet_model_2.add(Dropout(0.2))\n",
    "alexNet_model_2.add(Dense(128, activation='relu'))\n",
    "alexNet_model_2.add(Dropout(0.2))\n",
    "alexNet_model_2.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "alexNet_model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adamax(), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "    name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    "    name=\"AUC\",\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=True,\n",
    "    label_weights=None,\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_no=70\n",
    "alex_model_2=alexNet_model_2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_1, val_accuracy_1=alex_model_1.history['accuracy'], alex_model_1.history['val_accuracy']\n",
    "accuracy_2, val_accuracy_2=alex_model_2.history['accuracy'], alex_model_2.history['val_accuracy']\n",
    "accuracy_3, val_accuracy_3=alex_model.history['accuracy'], alex_model.history['val_accuracy']\n",
    "\n",
    "plt.plot(range(epoch_no), val_accuracy_2, label='Adamax', linestyle='-', color='blue')\n",
    "plt.plot(range(epoch_no), val_accuracy_1, label='Adam', linestyle='-', color='red')\n",
    "plt.plot(range(epoch_no), val_accuracy_3, label='Adam d=0.2', linestyle='-', color='green')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.title('Variation of test accuracy by dropout rate in a binary classification task')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(prev_layer):\n",
    "    \n",
    "    conv1=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n",
    "    conv1=BatchNormalization()(conv1)\n",
    "    conv1=Activation('relu')(conv1)\n",
    "    \n",
    "    conv3=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n",
    "    conv3=BatchNormalization()(conv3)\n",
    "    conv3=Activation('relu')(conv3)\n",
    "    conv3=Conv1D(filters = 64, kernel_size = 3, padding = 'same')(conv3)\n",
    "    conv3=BatchNormalization()(conv3)\n",
    "    conv3=Activation('relu')(conv3)\n",
    "    \n",
    "    conv5=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n",
    "    conv5=BatchNormalization()(conv5)\n",
    "    conv5=Activation('relu')(conv5)\n",
    "    conv5=Conv1D(filters = 64, kernel_size = 5, padding = 'same')(conv5)\n",
    "    conv5=BatchNormalization()(conv5)\n",
    "    conv5=Activation('relu')(conv5)\n",
    "    \n",
    "    pool= MaxPool1D(pool_size=3, strides=1, padding='same')(prev_layer)\n",
    "    convmax=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(pool)\n",
    "    convmax=BatchNormalization()(convmax)\n",
    "    convmax=Activation('relu')(convmax)\n",
    "    \n",
    "    layer_out = concatenate([conv1, conv3, conv5, convmax], axis=1)\n",
    "    \n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_model(input_shape):\n",
    "    X_input=Input(input_shape)\n",
    "    \n",
    "    X = ZeroPadding1D(padding=3)(X_input)\n",
    "    \n",
    "    X = Conv1D(filters = 64, kernel_size = 7, padding = 'same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPool1D(pool_size=3, strides=2, padding='same')(X)\n",
    "    \n",
    "    X = Conv1D(filters = 64, kernel_size = 1, padding = 'same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = inception_block(X)\n",
    "    X = inception_block(X)\n",
    "    \n",
    "    X = MaxPool1D(pool_size=7, strides=2, padding='same')(X)\n",
    "    \n",
    "    X = GlobalAveragePooling1D()(X)\n",
    "    X = Dense(5,activation='sigmoid')(X)\n",
    "    \n",
    "    incep_model = tf.keras.Model(inputs = X_input, outputs = X, name='Inception')\n",
    "    \n",
    "    return incep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = inception_model(input_shape = (200,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " inception_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        multi_label=True,\n",
    "        label_weights=None,\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inception_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4= tf.keras.Sequential()\n",
    "\n",
    "model_4.add(layers.Input(shape=(200,12)))\n",
    "#model.add(layers.Permute((2, 1)))\n",
    "\n",
    "model_4.add(layers.Conv1D(filters=32, kernel_size=64, strides=1, padding='same'))\n",
    "model_4.add(layers.LeakyReLU())\n",
    "\n",
    "model_4.add(layers.Dropout(0.2))\n",
    "\n",
    "model_4.add(layers.Conv1D(filters=64, kernel_size=64, strides=1, padding='same'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model_4.add(layers.LeakyReLU())\n",
    "model_4.add(layers.Dropout(0.2))\n",
    "\n",
    "model_4.add(layers.MaxPool1D(pool_size=2))\n",
    "\n",
    "model_4.add(layers.Conv1D(filters=128, kernel_size=64, strides=1, padding='same'))\n",
    "model_4.add(layers.LeakyReLU())\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "model_4.add(layers.Dropout(0.2))\n",
    "\n",
    "model_4.add(layers.Conv1D(filters=256, kernel_size=64, strides=1, padding='same'))\n",
    "model_4.add(layers.LeakyReLU())\n",
    "model_4.add(layers.Dropout(0.2))\n",
    "\n",
    "model_4.add(layers.Conv1D(filters=512, kernel_size=64, strides=1, padding='same'))\n",
    "model_4.add(layers.LeakyReLU())\n",
    "model_4.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "model_4.add(layers.MaxPool1D(pool_size=2))\n",
    "\n",
    "model_4.add(layers.Flatten())\n",
    "\n",
    "model_4.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "    name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    "    name=\"AUC\",\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=True,\n",
    "    label_weights=None,\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 48, 96)            12768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 48, 96)            384       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 48, 96)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 24, 96)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 24, 256)           123136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 24, 256)           1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 24, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 12, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 12, 384)           295296    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 384)           1536      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 12, 384)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 10, 384)           442752    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 10, 384)           1536      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 10, 384)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 8, 256)            295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 256)            1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8, 128)            65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 128)            512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,274,469\n",
      "Trainable params: 1,271,461\n",
      "Non-trainable params: 3,008\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(layers.Convolution1D(filters=96, kernel_size=11, strides=4, input_shape=(200,12)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(layers.Convolution1D(filters=256, kernel_size=5, padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(layers.Convolution1D(filters=384, padding='same', kernel_size=3))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "\n",
    "model.add(layers.Convolution1D(filters=384, kernel_size=3))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "\n",
    "model.add(layers.Convolution1D(filters=256, kernel_size=3))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "\n",
    "model.add(layers.Convolution1D(filters=128, kernel_size=2, padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "\n",
    "\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.LeakyReLU(alpha=0.001))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(5, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adamax(learning_rate=0.0003), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "            tf.keras.metrics.AUC(\n",
    "num_thresholds=200,\n",
    "curve=\"ROC\",\n",
    "summation_method=\"interpolation\",\n",
    "name=\"AUC\",\n",
    "dtype=None,\n",
    "thresholds=None,\n",
    "multi_label=True,\n",
    "label_weights=None,\n",
    ")])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real = pd.read_csv('Y_10s_superclass.csv')\n",
    "Y_real=np.array(Y_real)\n",
    "Y_real=Y_real[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 1 1]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "A=Y_real[:10,:]\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.unique(A, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 1 1]\n",
      " [1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_labels(labels, batch_size):\n",
    "    index=np.random.randint(labels.shape[0], size=batch_size)\n",
    "    return labels[index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=random_labels(A, 5)\n",
    "a,b=x[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real = pd.read_csv('Y_10s_superclass.csv')\n",
    "Y_real=np.array(Y_real)\n",
    "Y_real=Y_real[:1000,:]\n",
    "Y_unique = np.unique(Y_real, axis=0)\n",
    "\n",
    "X_real = np.loadtxt('X_10s_1000.csv')\n",
    "X_real = X_real.reshape(X_real.shape[0], 1000, 1)\n",
    "data = tf.data.Dataset.from_tensor_slices((X_real,Y_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TensorSliceDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gh/gpxxnks937l1w5hscl98fgzm0000gn/T/ipykernel_46534/442949034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'TensorSliceDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
